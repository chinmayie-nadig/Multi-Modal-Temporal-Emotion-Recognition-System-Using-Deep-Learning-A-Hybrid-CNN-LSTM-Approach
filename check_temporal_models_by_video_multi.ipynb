{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp #face detector\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pth_processing(fp):\n",
    "    class PreprocessInput(torch.nn.Module):\n",
    "        def init(self):\n",
    "            super(PreprocessInput, self).init()\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.to(torch.float32)\n",
    "            x = torch.flip(x, dims=(0,))\n",
    "            x[0, :, :] -= 91.4953\n",
    "            x[1, :, :] -= 103.8827\n",
    "            x[2, :, :] -= 131.0912\n",
    "            return x\n",
    "\n",
    "    def get_img_torch(img):\n",
    "        ttransform = transforms.Compose([\n",
    "            transforms.PILToTensor(),\n",
    "            PreprocessInput()\n",
    "        ])\n",
    "        img = img.resize((224, 224), Image.Resampling.NEAREST)\n",
    "        img = ttransform(img)\n",
    "        img = torch.unsqueeze(img, 0)\n",
    "        return img\n",
    "    return get_img_torch(fp)\n",
    "\n",
    "def norm_coordinates(normalized_x, normalized_y, image_width, image_height):\n",
    "    x_px = min(math.floor(normalized_x * image_width), image_width - 1)\n",
    "    y_px = min(math.floor(normalized_y * image_height), image_height - 1)\n",
    "    return x_px, y_px\n",
    "\n",
    "def get_box(fl, w, h):\n",
    "    idx_to_coors = {}\n",
    "    for idx, landmark in enumerate(fl.landmark):\n",
    "        landmark_px = norm_coordinates(landmark.x, landmark.y, w, h)\n",
    "        if landmark_px:\n",
    "            idx_to_coors[idx] = landmark_px\n",
    "\n",
    "    x_min = np.min(np.asarray(list(idx_to_coors.values()))[:,0])\n",
    "    y_min = np.min(np.asarray(list(idx_to_coors.values()))[:,1])\n",
    "    endX = np.max(np.asarray(list(idx_to_coors.values()))[:,0])\n",
    "    endY = np.max(np.asarray(list(idx_to_coors.values()))[:,1])\n",
    "\n",
    "    (startX, startY) = (max(0, x_min), max(0, y_min))\n",
    "    (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
    "    \n",
    "    return startX, startY, endX, endY\n",
    "\n",
    "def display_EMO_PRED(img, box, label='', color=(128, 128, 128), txt_color=(255, 255, 255), line_width=2):\n",
    "    lw = line_width or max(round(sum(img.shape) / 2 * 0.003), 2)\n",
    "    text2_color = (255, 0, 255)\n",
    "    p1, p2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
    "    cv2.rectangle(img, p1, p2, text2_color, thickness=lw, lineType=cv2.LINE_AA)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    tf = max(lw - 1, 1)\n",
    "    text_fond = (0, 0, 0)\n",
    "    text_width_2, text_height_2 = cv2.getTextSize(label, font, lw / 3, tf)\n",
    "    text_width_2 = text_width_2[0] + round(((p2[0] - p1[0]) * 10) / 360)\n",
    "    center_face = p1[0] + round((p2[0] - p1[0]) / 2)\n",
    "\n",
    "    cv2.putText(img, label,\n",
    "                (center_face - round(text_width_2 / 2), p1[1] - round(((p2[0] - p1[0]) * 20) / 360)), font,\n",
    "                lw / 3, text_fond, thickness=tf, lineType=cv2.LINE_AA)\n",
    "    cv2.putText(img, label,\n",
    "                (center_face - round(text_width_2 / 2), p1[1] - round(((p2[0] - p1[0]) * 20) / 360)), font,\n",
    "                lw / 3, text2_color, thickness=tf, lineType=cv2.LINE_AA)\n",
    "    return img\n",
    "\n",
    "def display_FPS(img, text, margin=1.0, box_scale=1.0):\n",
    "    img_h, img_w, _ = img.shape\n",
    "    line_width = int(min(img_h, img_w) * 0.001)\n",
    "    thickness = max(int(line_width / 3), 1)\n",
    "\n",
    "    font_face = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_color = (0, 0, 0)\n",
    "    font_scale = thickness / 1.5\n",
    "\n",
    "    t_w, t_h = cv2.getTextSize(text, font_face, font_scale, None)[0]\n",
    "\n",
    "    margin_n = int(t_h * margin)\n",
    "    sub_img = img[0 + margin_n: 0 + margin_n + t_h + int(2 * t_h * box_scale),\n",
    "              img_w - t_w - margin_n - int(2 * t_h * box_scale): img_w - margin_n]\n",
    "\n",
    "    white_rect = np.ones(sub_img.shape, dtype=np.uint8) * 255\n",
    "\n",
    "    img[0 + margin_n: 0 + margin_n + t_h + int(2 * t_h * box_scale),\n",
    "    img_w - t_w - margin_n - int(2 * t_h * box_scale):img_w - margin_n] = cv2.addWeighted(sub_img, 0.5, white_rect, .5, 1.0)\n",
    "\n",
    "    cv2.putText(img=img,\n",
    "                text=text,\n",
    "                org=(img_w - t_w - margin_n - int(2 * t_h * box_scale) // 2,\n",
    "                     0 + margin_n + t_h + int(2 * t_h * box_scale) // 2),\n",
    "                fontFace=font_face,\n",
    "                fontScale=font_scale,\n",
    "                color=font_color,\n",
    "                thickness=thickness,\n",
    "                lineType=cv2.LINE_AA,\n",
    "                bottomLeftOrigin=False)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Emotion Percentages:\n",
      "Anger: 39.75%\n",
      "Neutral: 29.61%\n",
      "Sadness: 1.42%\n",
      "Happiness: 21.91%\n",
      "Fear: 0.97%\n",
      "Surprise: 3.43%\n",
      "Disgust: 2.91%\n",
      "\n",
      "Overall Interest Level: Highly Disengaged\n",
      "Explanation: Showing strong signs of low interest or negative emotions\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_emotion_percentages(emotion_counts):\n",
    "    \"\"\"\n",
    "    Calculate percentage of each emotion detected.\n",
    "    \"\"\"\n",
    "    total_detections = sum(emotion_counts.values())\n",
    "    if total_detections == 0:\n",
    "        return {}\n",
    "    \n",
    "    emotion_percentages = {\n",
    "        emotion: round((count / total_detections) * 100, 2) \n",
    "        for emotion, count in emotion_counts.items()\n",
    "    }\n",
    "    return emotion_percentages\n",
    "\n",
    "def classify_interest_level(emotion_percentages):\n",
    "    \"\"\"\n",
    "    Classify interest level based on emotion percentages.\n",
    "    \"\"\"\n",
    "    # Define weights for different emotions\n",
    "    interest_weights = {\n",
    "        'Neutral': 0.5,\n",
    "        'Happiness': 1.0,\n",
    "        'Surprise': 0.7,\n",
    "        'Sadness': -0.8,\n",
    "        'Fear': -1.0,\n",
    "        'Disgust': -1.0,\n",
    "        'Anger': -0.9\n",
    "    }\n",
    "    \n",
    "    # Calculate weighted interest score\n",
    "    interest_score = sum(\n",
    "        interest_weights.get(emotion, 0) * percentage \n",
    "        for emotion, percentage in emotion_percentages.items()\n",
    "    )\n",
    "    \n",
    "    # Classify interest level with more detailed descriptions\n",
    "    if interest_score > 0.5:\n",
    "        return \"Highly Engaged\", \"Showing high enthusiasm and  positive emotional response\", (0, 255, 0)\n",
    "    elif interest_score > 0:\n",
    "        return \"Moderately Engaged\", \"Displaying mild interest and positive signals\", (0, 200, 255)\n",
    "    elif interest_score == 0:\n",
    "        return \"Neutral\", \"Maintaining a balanced, non-committal emotional state\", (128, 128, 128)\n",
    "    elif interest_score > -0.5:\n",
    "        return \"Slightly Disengaged\", \"Exhibiting reduced interest and potential discomfort\", (255, 165, 0)\n",
    "    else:\n",
    "        return \"Highly Disengaged\", \"Showing strong signs of low interest or negative emotions\", (0, 0, 255)\n",
    "\n",
    "def create_emotion_bar(emotion_percentages, width=300, height=200):\n",
    "    \"\"\"\n",
    "    Create a bar graph of emotion percentages.\n",
    "    \"\"\"\n",
    "    # Create a white background\n",
    "    bar_graph = np.ones((height, width, 3), dtype=np.uint8) * 255\n",
    "    \n",
    "    # Define colors for emotions\n",
    "    emotion_colors = {\n",
    "        'Neutral': (128, 128, 128),\n",
    "        'Happiness': (0, 255, 0),\n",
    "        'Sadness': (255, 0, 0),\n",
    "        'Surprise': (255, 165, 0),\n",
    "        'Fear': (128, 0, 128),\n",
    "        'Disgust': (75, 0, 130),\n",
    "        'Anger': (255, 0, 255)\n",
    "    }\n",
    "    \n",
    "    # Calculate bar heights\n",
    "    max_percentage = max(emotion_percentages.values()) if emotion_percentages else 100\n",
    "    bar_width = width // len(emotion_percentages)\n",
    "    \n",
    "    # Draw bars\n",
    "    for i, (emotion, percentage) in enumerate(emotion_percentages.items()):\n",
    "        bar_height = int((percentage / max_percentage) * (height - 50))\n",
    "        color = emotion_colors.get(emotion, (200, 200, 200))\n",
    "        \n",
    "        # Draw bar\n",
    "        start_x = i * bar_width\n",
    "        cv2.rectangle(bar_graph, \n",
    "                      (start_x, height - bar_height), \n",
    "                      (start_x + bar_width - 5, height), \n",
    "                      color, -1)\n",
    "        \n",
    "        # Add text\n",
    "        cv2.putText(bar_graph, \n",
    "                    f'{emotion}\\n{percentage}%', \n",
    "                    (start_x, height - bar_height - 10), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 1)\n",
    "    \n",
    "    return bar_graph\n",
    "\n",
    "def main():\n",
    "    # Previous initialization code remains the same\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    name_backbone_model = '0_66_49_wo_gl'\n",
    "    name_LSTM_model = 'IEMOCAP'\n",
    "    \n",
    "    # Load models (previous code)\n",
    "    pth_backbone_model = torch.jit.load(r'C:\\Users\\prajw\\Documents\\Codefinalyearproject\\Final_code_large\\EMO-AffectNetModel\\tf\\torchscript_model_0_66_37_wo_gl.pth')\n",
    "    pth_backbone_model.eval()\n",
    "    \n",
    "    pth_LSTM_model = torch.jit.load(r'C:\\Users\\prajw\\Documents\\Codefinalyearproject\\Final_code_large\\EMO-AffectNetModel\\tf\\Aff-Wild2.pth')\n",
    "    pth_LSTM_model.eval()\n",
    "    \n",
    "    DICT_EMO = {0: 'Neutral', 1: 'Happiness', 2: 'Sadness', 3: 'Surprise', 4: 'Fear', 5: 'Disgust', 6: 'Anger'}\n",
    "    \n",
    "    # Initialize emotion tracking\n",
    "    emotion_counts = defaultdict(int)\n",
    "    \n",
    "    # Increase frame size\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = np.round(cap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    path_save_video = 'result_multi.mp4'\n",
    "    vid_writer = cv2.VideoWriter(path_save_video, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "    \n",
    "    # Dictionary to store LSTM features for each face\n",
    "    face_features = {}\n",
    "    \n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        max_num_faces=20,\n",
    "        refine_landmarks=False,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    ) as face_mesh:\n",
    "        while cap.isOpened():\n",
    "            t1 = time.time()\n",
    "            success, frame = cap.read()\n",
    "            if frame is None:\n",
    "                break\n",
    "            \n",
    "            # Create a semi-transparent overlay for better readability\n",
    "            overlay = frame.copy()\n",
    "            cv2.rectangle(overlay, (0, 0), (400, h), (200, 200, 200), -1)\n",
    "            cv2.addWeighted(overlay, 0.5, frame, 0.5, 0, frame)\n",
    "            \n",
    "            frame_copy = frame.copy()\n",
    "            frame_copy.flags.writeable = False\n",
    "            frame_copy = cv2.cvtColor(frame_copy, cv2.COLOR_BGR2RGB)\n",
    "            results = face_mesh.process(frame_copy)\n",
    "            frame_copy.flags.writeable = True\n",
    "            \n",
    "            if results.multi_face_landmarks:\n",
    "                # Process each detected face\n",
    "                for face_idx, fl in enumerate(results.multi_face_landmarks):\n",
    "                    startX, startY, endX, endY = get_box(fl, w, h)\n",
    "                    cur_face = frame_copy[startY:endY, startX:endX]\n",
    "                    \n",
    "                    # Process face with the model\n",
    "                    cur_face = pth_processing(Image.fromarray(cur_face))\n",
    "                    features = torch.nn.functional.relu(pth_backbone_model.extract_features(cur_face)).cpu().detach().numpy()\n",
    "                    \n",
    "                    # Initialize or update LSTM features for this face\n",
    "                    if face_idx not in face_features:\n",
    "                        face_features[face_idx] = [features] * 10\n",
    "                    else:\n",
    "                        face_features[face_idx] = face_features[face_idx][1:] + [features]\n",
    "                    \n",
    "                    # Process with LSTM\n",
    "                    lstm_f = torch.from_numpy(np.vstack(face_features[face_idx]))\n",
    "                    lstm_f = torch.unsqueeze(lstm_f, 0)\n",
    "                    output = pth_LSTM_model(lstm_f).cpu().detach().numpy()\n",
    "                    \n",
    "                    cl = np.argmax(output)\n",
    "                    label = DICT_EMO[cl]\n",
    "                    \n",
    "                    # Track emotion counts\n",
    "                    emotion_counts[label] += 1\n",
    "                    \n",
    "                    # Display emotion prediction on frame with improved bounding box\n",
    "                    cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 3)\n",
    "                    cv2.putText(frame, label, (startX, startY-10), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "            \n",
    "            # Calculate emotion percentages\n",
    "            emotion_percentages = calculate_emotion_percentages(emotion_counts)\n",
    "            \n",
    "            # Classify interest level\n",
    "            interest_level, explanation, color = classify_interest_level(emotion_percentages)\n",
    "            \n",
    "            # Create emotion bar graph\n",
    "            emotion_bar = create_emotion_bar(emotion_percentages)\n",
    "            \n",
    "            # Resize and place emotion bar on the frame\n",
    "            emotion_bar_resized = cv2.resize(emotion_bar, (400, 200))\n",
    "            frame[h-250:h-50, 0:400] = emotion_bar_resized\n",
    "            \n",
    "            # Display interest level with enhanced styling\n",
    "            cv2.rectangle(frame, (0, 0), (400, 50), color, -1)\n",
    "            cv2.putText(frame, f\"{interest_level}\", (10, 35), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "            \n",
    "            # Display explanation\n",
    "            cv2.putText(frame, explanation, (10, 80), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)\n",
    "            \n",
    "            t2 = time.time()\n",
    "            # FPS display\n",
    "            cv2.putText(frame, f'FPS: {1 / (t2 - t1):.1f}', (w-150, 50), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            vid_writer.write(frame)\n",
    "            cv2.imshow('Emotion Detection', frame)\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "        # Print final emotion percentages and interest level\n",
    "        print(\"\\nFinal Emotion Percentages:\")\n",
    "        for emotion, percentage in emotion_percentages.items():\n",
    "            print(f\"{emotion}: {percentage}%\")\n",
    "        \n",
    "        print(f\"\\nOverall Interest Level: {interest_level}\")\n",
    "        print(f\"Explanation: {explanation}\")\n",
    "        \n",
    "        vid_writer.release()\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()s\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
